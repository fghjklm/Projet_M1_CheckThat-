##########################################################################
Article lu: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
Lien : https://aclanthology.org/N19-1423.pdf
Path from seed (Clef Overview) : 29-11
Autheur: Joe Oleiwan

BERT : Bidirectional Encoder Representations from Transformers
INPUT: TEXT
OUTPUT: TEXT EMBEDDINGS

Dans les modèles comme chatGPT l'architecture est de gauche à droite, ce qui n'est pas bon pour des tâches au niveau de phrases car la prédiction d'un token se base sur le précédent.

On utilise la technique de MLM: masked language model qui va venir lever cette contrainte , qui va lui masquer aléatoirement quelques-uns des tokens dans l'input, et l'objectif sera de prédire l'id du vocabulaire des tokens masqués en se basant uniquement sur le contexte.

=> Cette méthode de MLM permet de fusionner le contexte de gauche et de droite dans une phrase.

En plus, on utilisera une tâche de prédiction de la phrase suivant qui va en même temps entrainer le modèle sur des représentations textuels en pairs.

##Voir schémas figure 1 sur article##

Mise à part la couche additionnelle en output, l'architecture est la même pour le modèle de préentrainement et de fine-tuning (ici pour répondre à une question => sQuAd)
vocab: 30 000 token dont 15% masked to predict
* Entrées : E
* CLS: le premier token est considéré spécial, c'est un token de classification
* Les pairs de phrases sont packetées en 1une avec un séparateur SEP
* La dernière couche caché correspond aux tokens masqués qui sont nourrit à une softmax sur le vocabulaire.

sQuaD: stanford question answering dataset
MNLI: Multigenre Natural language inference
NER: Named entity recognition

###########################################################################
Critique par rapport au projet de détéction de la subjectivité dans les news :
Récapitulatif : Le document "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" introduit BERT, un modèle 
de représentation linguistique profondément bidirectionnel conçu pour améliorer la compréhension du langage.

Points positifs :

    -Le modèle est opensource
    -Il est facilement flexible, pouvant être adapté à diverses tâches NLP avec un simple ajustement de la couche de sortie.
    

Points négatifs pour votre projet :

   -Ancien et fait objet de beaucoup de recherches déja établies avec des variantes de ce modèle, les reproduire ne sera pas créatif ou nouveau.
