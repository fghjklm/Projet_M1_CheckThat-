Article lu: FakeBERT: Fake news detection in social media with a BERT-based deep learning approach
Lien : https://link.springer.com/article/10.1007/s11042-020-10183-2
Auteur de la synthèse: Mouysset Martin

Papier détaillant un modèle nommé "FakeBert".

Fonctionnement: Un CNN en sortie d'un model BERT plus classique.

Le modèle BERT fait un embedding sur chaque mot d'une phrase.

Ensuite cet emdedding est entré dans trois couche de convolution en parrallèle (ayant un noyau de taille différente)

Les données sont ensuite concaténées puis passé dans une dernière couche de convolution.

Pour finir, deux couche de neurones denses sont utilisées pour la prédiction.

Détail des couches:

Layer 		Input size 				Output size 	Param number
Embedding 	1000 					1000 × 100	25187700
Conv1D 	1000 × 100 				998 × 128 	38528
Conv1D 	1000 × 100 				997 × 128 	51328
Conv1D 	1000 × 100 				996 × 128 	64128
Maxpool	998 × 128 				199 × 128 	0
Maxpool 	997 × 128 				199 × 128 	0
Maxpool 	996 × 128 				199 × 128 	0
Concatenate	199 × 128, 199 × 128, 199 × 128 	597 × 128 	0
Conv1D 	597 × 128 				593 × 128 	82048
Maxpool 	593 × 128 				118 × 128 	0
Conv1D 	118 × 128 				114 × 128 	82048
Maxpool 	114 × 128 				3 × 128 	0
Flatten 	3 × 128 				384 		0
Dense 		384 					128 		492

Hyperparamètres utilisés:

Hyperparameter 		Value
Number of convolution layers 	5
Number of max pooling layers 	5
Number of dense layers 	2
Number of Flatten layers 	1
Dropout rate 			0.2
Optimizer			Adadelta
Activation function 		Relu
Loss function 			Categorical-crossentropy
Number of epochs 		10
Batch size 			128

Performance observée (pour la detection de FakeNews): 
accuracy de 98.50
FP: 0.016
FN: 0.0059

Un autre bon modèle, quoiqu'ayant montré des résultats légèrement plus faible, est l'utilisation d'une couche de LSTM après les couches de convolution.


