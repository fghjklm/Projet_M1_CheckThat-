Article lu : FakeBERT: Fake news detection in social media with a BERT-based deep learning approach
Lien : https://link.springer.com/article/10.1007/s11042-020-10183-2
Path from seed (Clef Overview) : 29-15
Auteur de la synthèses: Oleiwan Joe

OBJECTIF: est d'utiliser BERT et un CNN pour détécter les fake news
MODELE : Un modèle deep learning basé sur BERT en combinant différents blocs parallèles du CNN de couche unique : FakeBERT with BERT

word embedding: GloVe, BERT
AUTRES MODELES : DeepLearning : CNN with BERT, LSTM with GloVe, LSTM with BERT
				 MachineLearning: MNB with GloVe(0.89), KNN with GloVe, DT with GloVe (0.73), RF with GloVe

-[5] Results :
RESULTAT
Performance observée (pour la detection de FakeNews): 
Précision : 0.989 
FP: 0.016
FN: 0.0059


     |		obj predit      |	  	subj predit
-----|--------------------------|-----------------------------------
obj  |		TN: 1045  	|		FP: 6
-----|--------------------------|-----------------------------------
subj |		FN: 17 		| 		TP: 1012


-[Fig 5] Le modèle FakeBERT:
									  	(3 parallel blocks)
               						| 1D-CNN Maxpool |  
					INPUT: Embeddings |	1D-CNN Maxpool	| 1D-CNN Maxpool | 1D-CNN Maxpool | Dense 1 | Dense 2 | OUTPUT: classe obj ou subj
									  	| 1D-CNN Maxpool |
-[3.6] Proposed model: FakeBERT
Couche de convolution : 
	Elle est constituée d'un ensemble de filtres et de noyaux visant à obtenir de meilleures représentations sémantiques de mots de longueurs différentes. Les principales opérations consistent en des multiplications matricielles, suivies d'une fonction d'activation pour produire la sortie finale. Notre modèle utilise trois blocs parallèles de 1D-CNN, chacun ayant une couche dans chaque bloc et deux couches simples après le processus de concaténation, avec des tailles de noyau et des filtres différents.

Couche de mise en commun maximale : 
	Cette couche réduit efficacement la sortie de la couche de convolution et diminue le nombre d'opérations de calcul nécessaires. Sa fonction est de réduire progressivement la taille spatiale de la représentation afin de diminuer la quantité de paramètres et de calcul dans le réseau. Notre modèle utilise cinq couches de mise en commun maximale, dont trois dans des blocs parallèles de 1D-CNN et deux dans des couches de convolution simples.

Couche d'aplanissement : 
	Placée entre la couche de convolution et la couche entièrement connectée, cette couche transforme une matrice bidimensionnelle de caractéristiques en un vecteur pouvant être utilisé par un classificateur de réseau neuronal entièrement connecté.

Couche dense : 
	Cette couche, composée d'une série de neurones, reçoit une entrée de tous les neurones de la couche précédente, établissant ainsi une connexion dense. Elle est caractérisée par une matrice de poids, un vecteur de biais et les activations de la couche précédente. Dans de nombreuses méthodes antérieures, une ou deux couches denses ont principalement été utilisées dans les réseaux proposés pour éviter le surajustement. Notre modèle comprend également deux couches denses, chacune comportant un nombre variable de filtres.

Dropout : 
	Cette technique de régularisation consiste à ignorer aléatoirement certains neurones pendant l'apprentissage. Sa principale contribution à l'activation des neurones en aval est temporairement supprimée lors de la passe avant, et les mises à jour des poids ne sont pas appliquées aux neurones lors de la passe arrière. Dans notre modèle, le dropout est appliqué aux couches denses du réseau, où les arêtes sortantes des unités cachées sont aléatoirement définies à 0 à chaque mise à jour de la phase d'entraînement. Une valeur de dropout de 0,2 est utilisée dans ces expériences.

Fonction d'activation ReLU : 
	La fonction ReLU (Rectifier Unit) est la plus couramment utilisée pour les sorties des neurones de CNN. Son principal avantage par rapport à d'autres fonctions d'activation réside dans son comportement sélectif, n'activant pas simultanément tous les neurones. Dans notre modèle, ReLU est calculé après la convolution et est une fonction d'activation non linéaire, tout comme tanh ou sigmoid.

Fonction de perte (L) : 
	L'entropie croisée compare la prédiction du modèle avec l'étiquette, qui représente la distribution de probabilité réelle. L'entropie croisée diminue à mesure que la prédiction devient plus précise, atteignant zéro en cas de prédiction parfaite. Cette fonction de perte est utilisée pour l'entraînement d'un modèle de classification. En cas de classification binaire, l'entropie croisée peut être calculée comme suit :

	L = − (ylog(p) + (1 − y)log(1 − p)) (2)

Pour une classification avec plus de deux classes, une perte distincte est calculée pour chaque étiquette de classe par observation et les résultats sont sommés.

-[Table 5]
Détail des couches:

Layer 		Input size 				Output size 	Param number
Embedding 	1000 					1000 × 100	25187700
Conv1D 		1000 × 100 				998 × 128 	38528
Conv1D 		1000 × 100 				997 × 128 	51328
Conv1D 		1000 × 100 				996 × 128 	64128
Maxpool		998 × 128 				199 × 128 	0
Maxpool 	997 × 128 				199 × 128 	0
Maxpool 	996 × 128 				199 × 128 	0
Concatenate	199 × 128, 199 × 128, 199 × 128 	597 × 128 	0
Conv1D 		597 × 128 				593 × 128 	82048
Maxpool 	593 × 128 				118 × 128 	0
Conv1D 		118 × 128 				114 × 128 	82048
Maxpool 	114 × 128 				3 × 128 	0
Flatten 	3 × 128 				384 		0
Dense 		384 					128 		49280
Dense 		128 					2 		258

-[Table 8]
Hyperparamètres utilisés:

Hyperparameter 		Value
Number of convolution layers 	5
Number of max pooling layers 	5
Number of dense layers 	2
Number of Flatten layers 	1
Dropout rate 			0.2
Optimizer			Adadelta
Activation function 		Relu
Loss function 			Categorical-crossentropy
Number of epochs 		10
Batch size 			128


##########################################################################################################################################################################
Critique par rapport au projet de détéction de la subjectivité dans les news :
Récapitulatif: Le document "FakeBERT: Fake News Detection in Social Media with a BERT-Based Deep Learning Approach" décrit une méthode de détection de fausses nouvelles en utilisant une combinaison de BERT et de réseaux de neurones convolutionnels pour améliorer l'exactitude de la classification.

Points positifs :

   
    -Résultats de plusieurs modèles machine learning et deep learning fournis.
    -Le document fournit une analyse exhaustive des performances et de l'architecture fakeBert utilisée, ce qui pourrait enrichir l'approche méthodologique de notre projet.
    -La précision de 0,989 indique que le modèle est très performant pour une tâche de classification.

Points négatifs :
    
    - Les caractéristiques linguistiques qui déterminent la subjectivité peuvent différer de celles utilisées pour juger de la véracité. Il faudrait réexaminer et potentiellement reconfigurer le modèle pour qu’il puisse distinguer correctement entre subjectivité et objectivité.
    - La mise en œuvre de modèles aussi complexes pourrait nécessiter des ressources importantes et une expertise technique approfondie.
    - Il faut considérer les risques de surajustement sur notre ensemble de données.
