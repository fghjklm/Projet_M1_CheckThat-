##########################################################################
Informations sur BERT
Articles de base: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
Lien : https://aclanthology.org/N19-1423.pdf
Path from seed (Clef Overview) : 29-11
Autheur: Joe Oleiwan

BERT : Bidirectional Encoder Representations from Transformers
INPUT: TEXT
OUTPUT: TEXT EMBEDDINGS

Dans les modèles comme chatGPT l'architecture est de gauche à droite, ce qui n'est pas bon pour des tâches au niveau de phrases car la prédiction d'un token se base sur le précédent.

On utilise la technique de MLM: masked language model qui va venir lever cette contrainte , qui va lui masquer aléatoirement quelques-uns des tokens dans l'input, et l'objectif sera de prédire l'id du vocabulaire des tokens masqués en se basant uniquement sur le contexte.

=> Cette méthode de MLM permet de fusionner le contexte de gauche et de droite dans une phrase.

En plus, on utilisera une tâche de prédiction de la phrase suivant qui va en même temps entrainer le modèle sur des représentations textuels en pairs.

##Voir schémas figure 1 sur article##

Mise à part la couche additionnelle en output, l'architecture est la même pour le modèle de préentrainement et de fine-tuning (ici pour répondre à une question => sQuAd)
vocab: 30 000 token dont 15% masked to predict
* Entrées : E
* CLS: le premier token est considéré spécial, c'est un token de classification
* Les pairs de phrases sont packetées en 1une avec un séparateur SEP
* La dernière couche caché correspond aux tokens masqués qui sont nourrit à une softmax sur le vocabulaire.

sQuaD: stanford question answering dataset
MNLI: Multigenre Natural language inference
NER: Named entity recognition

###########################################################################s