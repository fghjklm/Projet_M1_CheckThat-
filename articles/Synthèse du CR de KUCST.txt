SynthÃ¨se du CR de KUCST
https://arxiv.org/pdf/2306.09108.pdf


- RÃ©sultats :
Anglais : 4

- ModÃ¨les :
K-Nearest Neighbors (ğ¾ = 5) -> worst
Logistic Regression
Linear Support Vector Machine
Multilayer Perceptron
Decision Tree
Random Forest
Gradient Boosting -> best


- Fine-tuning :
All models are trained using a Train/test validation procedure, where two thirds of
the data are used for training and the remaining part is used for testing (0.66 vs 0.33)


- Traitement des donnÃ©es
Plusieurs techniques de tokenisation:
    - Word-level Bag-of-words (chaque mot)
    - TF-IDF Weighted Bag-of-Words (attribution de poids en fonction de l'importance d'un mot et de sa frÃ©quence) -> poor results
    - Character-level Bag-of-Words (sÃ©quence de 1-4 caractÃ¨re)
    - POS Tagg Bag-of-Word (Ã©tiquettes grammaticales 1-4 lettres)
    - Morphological features (analyse d'utiÃ©s lexicales et de leurs structures??)
    - BERT encoding

- ProblÃ¨mes rencontrÃ©s



- critique:
ModÃ¨les simples Ã  implÃ©menter.

Effectivement rÃ©sultats mÃ©diocres (4Ã¨me en anglais alors que seule langue traitÃ©e) mais permet de voir que l'utilisation de LLM (BERT) a Ã©tÃ© bien plus efficace (surement une bonne piste)
Bonne explication des techniques de tokenisation -> rÃ©sumÃ©s peuvent Ãªtre utiles puisqu'on as pas encore les connaissances.


