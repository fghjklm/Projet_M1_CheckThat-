Lu : https://ceur-ws.org/Vol-3497/paper-027.pdf

NN at CheckThat! 2023: Subjectivity in News Articles
Classification with Transformer Based Models

informations:
(1)
  -classification binaire
  -4 problèmes communéments rencontrés:
    -les datasets sont biaisés à l'encontre de certain types de textes desquels ils sont majoritairement issus (dur de généraliser)
    -les modèles se basent sur des annotations manuelles, chères et longues à créer
    -les systèmes existant ont du mal à différencier les nuances de la langue, tel que le sarcasme et l'ironie
    -ils sont habituelement incapable de gérer différentes langues

  -modèles utilisés:
    - fine-tuned XLM-RoBERTa large 
    - BERT
    - BERT multilingual(BERT-m) 
    
  - macro-F1 scores entre 0.71 et 0.81 sur toutes les langues, modèles plutôt efficaces
  
(2)
    
  XLM-RoBERTa large a eu les meilleurs résultats, voir : http://arxiv.org/pdf/1911.02116.pdf (A FAIRE)

(3) 

les données ont été pré-travaillée pour pouvoir être utilisées plus efficacement, même si ça n'a que très légèrement changé les résultats.
Au final, le modèle XLM-RoBERTa avait 59,892,482 paramètres entrainables

Au final, le modèle XLM-RoBERTa est le plus efficace, on pourrait se concentrer dessus pour voir ce qu'il est faisable.


Unsupervised Cross-lingual Representation Learning at Scale

lien :http://arxiv.org/abs/1911.02116

Article détaillant le modèle XLM-RoBERTa.
C'est un modèle très efficace pour du multilangue, pas forcément ce qui nous interesse, mais plutôt très efficace sur des jeux de données faible.
