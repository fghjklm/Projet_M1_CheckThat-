Synthèse du CR d'Accenture:
https://ceur-ws.org/Vol-3497/paper-045.pdf


- Résultats:
Arabic: 3 +
Dutch: 5 -
English: 8 -
German: 7 -
Italian: 3 +
Turkish: 4 +



- Modèle(s): 
BERT et RoBERTa

Arabic, we used lanwuwei/GigaBERT-v4-Arabic-and-English
English, we used roberta-large
Turkish, we used dbmdz/bert-base-turkish-cased
German, we used dbmdz/bert-basegerman-uncased
Italian, we used dbmdz/bert-base-italian-xxl-uncased
Dutch, we used GroNLP/bert-base-dutch-cased

we chose the uncased variant for any new model




- Fine-Tuning:
expérimentation et à la comparaison avec roberta-large: ils ont également fine-tuné un modèle (cffl/bert-base-styleclassification-subjective-neutral) sur une tâche de classification de subjectivité/style. 

"fine-tune" lanwuwei/GigaBERT-v4-Arabic-and-English at different levels of data augmentation and compare performances on the gold test set provided
ils ont simplement envoyé dans ce modèle différents niveaux de back-translation pour voir les résultats et scores.

sur BERT et RoBERTa, ajout de deux couches juste avant la couche finale de classification: mean-pooling et dropout pour essayer d'empêcher l'over-fitting.
Utilisation d'un adam Optimizer avec ces paramètres: learning rate: 2e-5, epsilon: 1.5e-8.
Utilisation d'une loss fonction "cross-entropy" avec ces paramètres: 4 epochs, batch size: 32.



- Traitement des données: 
WordPiece tokenization -> pas explicite sur l'algorithme utilisé
mention de UNK (unknown token), ce sont des tokens ignorés par le lexicon, si le nombre est trop élevé on perd en précision, dans ce cas nombre assez faible (voir nul avec RoBERTa)
UNK: cf table 2 du document

data augmentation par back-translation par AWS translation. done on minority class (always subjective) et ajout des nouvelles phrases créées au dataset.
significant incrase in recal and F1-score for the positive class.

recherche d'un sweet spot pour le BLEU score (score de qualité de traduction):
trop bien traduit= même phrase qu'avant donc pas d'intérêt pour la data augmentation.
pas assez bien traduit= perte d'informations et de mots essentiels dans un cadre de subjectivité ou objectivité.

Back-translation effectuée à plusieurs niveaux, par exemple: AR>EN>AR ou AR>EN>ES>EN>AR.
Table 9 montre le score du modèle avec différents niveaux de back-translation seulement sur arabe. aucune info sur les autres langues.



- Problèmes rencontrés: 
Label bias (imbalance)

RoBERTa: moins bons résultats en général à cause du manque de données

vocabulaire à un rôle crucial dans la classification et cette méthode de data augment fait varier le voc et donc on peut perdre la SUBJ d'un texte (exemple dans pdf)


- critique:
meilleurs résutats: 3èmes sur arabe et italien -> pas super efficace (vrmt pas terrible ailleurs)
bonne explication détaillée de chaque modèle et technique utilisée
au vue des résultats, back-translation pas hyper efficace car reformulations peuvent perdre des "mots-clefs" qui font la subjectivité.



- Table 2: Unknown Token Distribution in Data for Each Language
Language Tokenizer Type Modeling Set WordPiece Unknown Token
Arabic BERT-based Training 43,601 3
Testing 16,050 8
Validation 11,286 3
Dutch BERT-based Training 19,033 3
Testing 10,997 0
Validation 4,902 0
English RoBERTa-based Training 24,147 0
Testing 7,674 0
Validation 6,935 0
German BERT-based Training 21,318 7
Testing 8,293 7
Validation 5,267 4
Italian BERT-based Training 41,767 2
Testing 14,978 2
Validation 5,277 0
Turkish BERT-based Training 16,593 5
Testing 4,795 4
Validation 4,008 2



- Table 6 Accenture’s Results From the CheckThat! 2023 Lab Task 2
Language Class Precision Recall F1-score
Arabic OBJ 0.936 0.810 0.869
SUBJ 0.473 0.756 0.582
macro avg 0.705 0.783 0.725
weighted avg 0.851 0.800 0.816
English OBJ 0.630 0.879 0.734
SUBJ 0.827 0.528 0.644
macro avg 0.728 0.703 0.689
weighted avg 0.733 0.696 0.687
Turkish OBJ 0.841 0.667 0.744
SUBJ 0.757 0.892 0.819
macro avg 0.799 0.779 0.781
weighted avg 0.796 0.788 0.784
German OBJ 1.000 0.005 0.010
SUBJ 0.335 1.000 0.501
macro avg 0.667 0.503 0.256
weighted avg 0.778 0.337 0.174
Italian OBJ 0.866 0.681 0.763
SUBJ 0.446 0.709 0.548
macro avg 0.656 0.695 0.655
weighted avg 0.754 0.689 0.706
Dutch OBJ 0.877 0.380 0.531
SUBJ 0.578 0.941 0.716
macro avg 0.728 0.661 0.623
weighted avg 0.735 0.646 0.618




- Table 7 Accenture’s Results from the CheckThat! 2023 Lab Task 2
Language Accuracy
Arabic 0.800
English 0.696
Turkish 0.788
German 0.337
Italian 0.689
Dutch 0.646


- Table 9 BERT-based Arabic Model Performance at Different Level of Data Augmentation.
Augmentation Accuracy
No augmentation 0.816
AR > EN > AR 0.823
AR > EN > AR, and 0.820
AR > EN > ES > EN > AR
AR > EN > AR, 0.800
AR > EN > ES > EN > AR, and
AR > EN > FR > EN > AR





